# Advanced Namespace Tools blog 

### 30 November 2018

## Design of the ANTS colony-spawngrid

This post describes work-in-progress. If you are interested in the current status, stop in to gridchat on the 9gridchan public grid.

### Overview

ANTS colony servers provide spawning and saving of snapshotted Plan 9 namespace environments on demand, from a globally replicated store of filesystem images. The goal is a simple Plan 9 analog of "cloud computing" platforms such as AWS and Google Compute Engine. The modern distributed architecture of containerized microservices is related to what Plan 9 pioneered in the 1990s - per process independent namespace (the foundation of container-like systems), and single purpose network services (delivered via 9p in Plan 9 as opposted to http-json) working together.

Plan 9 laid a technological and conceptual foundation for this approach to distributed systems, but in its own network architecture and administration, it mostly remained rooted in static pre-configuration, and single-point-of-failure services. The flexibility of per-process namespace was not generally used to create container-like systems. The ANTS project perspective is that container-like partitions of the namespace represent a natural evolutionary flow from Plan 9 design principles. No attempt has been made to imitate particular features of BSD jails or linux cgroups, but rather to evolve features based on real-world experience using Plan 9 systems in this way.

### Storage backend

The backing store of the grid has two components - a set of venti datablocks, and a database file in ndb format mapping rootscores, names for those rootscores, and users. By replicating this data between storage nodes, the grid services can spawn any root filesystem on any server on demand. Synchronization of venti blocks is performed by the ventiprog command. Each server stores in 9fat a server.info file for each other venti it wishes to replicate blocks, and then runs ventiprog -f server.info periodically for each of the .info files. This is efficient because only newly written blocks will be replicated. The venti protocol does not offer authentication and encryption, which we need to run a globally distributed network of venti servers. Fortunately, 9front provides general purpose tools for making authenticated and encrypted tunnels. Each venti server runs its venti listening on local loopback, and then has this script as /rc/bin/service/tcp27034:

	#!/bin/rc
	exec tlssrv -A /bin/aux/trampoline tcp!127.1!17034

This creates a listener which will authenticate clients and then tls-tunnel/proxy the connection to the venti listening on loopback. Clients need to start their own local listener to make the connection:

	aux/listen1 -t tcp!127.1!27035 /bin/tlsclient -a tcp!server.ip!27034 &

Now, with that listener in place on the client, the client sets a venti env variable like:

	venti=tcp!127.1!27035

Then commands such as the wrarena invoked by ventiprog will talk to the local listener, which will make the auth+tls connection to the remote server and tunnel the traffic.

The scores database is also stored in 9fat in the 'scorenames' file. A simple script, scorecopy, is used to sync this file between servers. Storage servers provide a /srv/fatsrv of their 9fat to make this more convenient. The scorecopy replication should match the ventiprog replication - each venti should have a matching set of blocks and corresponding scores. The current spawngrid doesn't enforce this perfectly; there may be a lag of a few minutes before the datablocks catch up to the score replication. No huge harm is caused by this but i should be tweaked.

### Spawning service

The spawngrid is organized in paired storage+cpu service units. 